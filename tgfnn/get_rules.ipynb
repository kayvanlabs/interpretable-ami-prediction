{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract rules and membership functions from TGFNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generalized_fuzzy_net import GeneralizedFuzzyClassifier as TGFNN\n",
    "from rule_extraction import draw_membership_function, extract_encoding_intervals, extract_relations\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import cal_acc\n",
    "\n",
    "def get_thresholds(rules, params, encoding_value_details):\n",
    "    rule_var_name = rules.index\n",
    "    rule_var_name = set([name.split('_low')[0] for name in rule_var_name if 'low' in name])\n",
    "\n",
    "    continous_variable_name = [params.feature_names[index] \\\n",
    "                                for index in range(len(params.category_info)) \\\n",
    "                                if params.category_info[index]==0]\n",
    "\n",
    "    encoding = pd.DataFrame(np.transpose(encoding_value_details), index=continous_variable_name,\n",
    "                            columns=['low', 'medium_left', 'medium_right', 'high'])\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "def extract_rules_from_net(net, params, scaler):\n",
    "    \n",
    "    row_names_continous = []\n",
    "    row_names_categorical = []\n",
    "    for i in range(len(params.category_info)):\n",
    "        if params.feature_names is None:\n",
    "            if params.category_info[i] == 0:\n",
    "                row_names_continous += [f'x{i}_low', f'x{i}_medium', f'x{i}_high']\n",
    "            else:\n",
    "                row_names_categorical += [f'x{i}_level{j}' for j in range(params.category_info[i])]\n",
    "        else:\n",
    "            feature_name = params.feature_names[i]\n",
    "            if params.category_info[i] == 0:\n",
    "                row_names_continous += [f'{feature_name}_low', f'{feature_name}_medium', f'{feature_name}_high']\n",
    "            else:\n",
    "                row_names_categorical += [f'{feature_name}_level{j}' for j in range(params.category_info[i])]\n",
    "    row_names = row_names_continous + row_names_categorical\n",
    "    \n",
    "    ## For continous variables\n",
    "    # Extract the way that varaibles are encoded\n",
    "    encoding_values, extract_encoding_details = extract_encoding_intervals(net, scaler)\n",
    "    encoding_column_continous = np.expand_dims(encoding_values.flatten('F'), axis=1)\n",
    "    encoding_column_categorical = []\n",
    "    category_levels = params.category_info[params.category_info>0]\n",
    "    for n_levels in category_levels:\n",
    "        encoding_column_categorical += [i for i in range(n_levels)]\n",
    "    encoding_column_categorical = np.expand_dims(np.array(encoding_column_categorical), axis=-1)\n",
    "    encoding_column = np.concatenate([encoding_column_continous,\n",
    "                                      encoding_column_categorical], axis=0)\n",
    "\n",
    "    # Extract the attention mask and connection mask\n",
    "    # the entry in relation is calculated by multiplying the corresponding entry in the attention mask with\n",
    "    # the corresponding entry in the connection mask     \n",
    "    attention_mask, connection_mask, relation_mat = extract_relations(net, params)\n",
    "\n",
    "    # Extract the output layer (why do I square it?)\n",
    "    out_layer = net.layer3.weight.detach().numpy()**2\n",
    "    if params.binary_pos_only:\n",
    "        row_names.append('direction')\n",
    "    else:\n",
    "        row_names.extend([f'{i}' for i in range(params.n_classes)])\n",
    "    \n",
    "    # Normalize the output layer\n",
    "    weighted_out_layer = out_layer/np.max(np.abs(out_layer))\n",
    "    out_row = np.insert(weighted_out_layer, 0, np.nan, axis=0)\n",
    "\n",
    "    rules = np.concatenate([encoding_column, relation_mat], axis=-1)\n",
    "    # rules = np.concatenate([rules, out_row.T], axis=0)\n",
    "    # print(out_row.shape)\n",
    "    rules = np.concatenate([rules, np.expand_dims(out_row, axis=0)], axis=0)\n",
    "        \n",
    "    # Build the row names and column names\n",
    "    if params.binary_pos_only:\n",
    "        all_column_names = ['encoding']\n",
    "        for i in range(params.n_rules):\n",
    "            all_column_names.append(f'Rule_{i}')\n",
    "    else:\n",
    "        all_column_names = ['encoding']\n",
    "        for i in range(params.n_rules):\n",
    "            all_column_names.append(f'Rule_{i}')\n",
    "            \n",
    "    # Table with all rules extracted from the network\n",
    "    rules_all =  pd.DataFrame(rules, columns=all_column_names, index=row_names)\n",
    "\n",
    "    return rules_all, attention_mask, connection_mask, extract_encoding_details, relation_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CV models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "random_state = 0\n",
    "dir_name = 'experiment'\n",
    "\n",
    "dataset = pickle.load(open(f'cv_results/{dir_name}/dataset.pkl', 'rb'))\n",
    "models = pickle.load(open(f'cv_output/experiment_60_all_GFN_cv_models.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    # evaluate\n",
    "    _, train_metrics, metric_names, _, _, fpr_train, tpr_train = cal_acc(model, dataset['X_train'], dataset['y_train'], model.n_classes>2)\n",
    "    _, test_metrics, _, _, _, fpr_test, tpr_test = cal_acc(model, dataset['X_test'], dataset['y_test'], model.n_classes>2)\n",
    "    # conf_mat = [conf_mat_train, conf_mat_test]\n",
    "\n",
    "    # compile metrics\n",
    "    roc_values = {'fpr_test': fpr_test, 'tpr_test': tpr_test, 'auc_test': test_metrics[5],\n",
    "                    'fpr_train': fpr_train, 'tpr_train': tpr_train, 'auc_train': train_metrics[5]}\n",
    "\n",
    "\n",
    "    metrics = pd.DataFrame([train_metrics, test_metrics], \n",
    "                            columns=metric_names, \n",
    "                            index=['Train', 'Test'])\n",
    "    metrics = metrics.reset_index(names='Set')\n",
    "    metrics['model_index'] = i\n",
    "    metric_names = metric_names + ['model_index']\n",
    "\n",
    "    # print(conf_mat[0])\n",
    "    # print(conf_mat[1])\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "metrics = pd.concat(all_metrics, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort_values(by=['auc'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_index(x):\n",
    "    concept_types = {\n",
    "        'pheno' : 'c',\n",
    "        'blood' : 'b',\n",
    "        'creatinine' : 'b',\n",
    "        'CO2' : 'b',\n",
    "        'BMI' : 'a',\n",
    "        'pulse' : 'a',\n",
    "        'height' : 'a',\n",
    "        'temperature' : 'a',\n",
    "        'calcium' : 'b',\n",
    "    }\n",
    "\n",
    "    # add new column of concept types based on if key substring found in index\n",
    "    for i in range(x.shape[0]):\n",
    "        concept = x.index.tolist()[i]\n",
    "        for key in concept_types.keys():\n",
    "            if key in concept:\n",
    "                x.loc[concept, 'Type'] = f'{concept_types[key]}{i}'\n",
    "                break\n",
    "    x = x.sort_values(by='Type', ascending=True)\n",
    "    x = x.drop(columns='Type')\n",
    "\n",
    "    return x\n",
    "\n",
    "def pub_fig_adjust_index(concepts, concept_thresholds):\n",
    "    '''\n",
    "    Use this function to adjust the names of concepts in the output rule table to make them more interpretable.\n",
    "    '''\n",
    "    new_concepts = []\n",
    "\n",
    "    for concept in concepts:\n",
    "        phenotype = False\n",
    "\n",
    "        if concept == 'directions' or concept.startswith('weight_'):\n",
    "            new_concepts.append(concept)\n",
    "            continue\n",
    "\n",
    "        # get threshholds\n",
    "        if concept[0].isdigit():\n",
    "            phenotype = True\n",
    "        elif '_level' not in concept:\n",
    "            threshholds = concept_thresholds.loc['_'.join(concept.split('_')[:-1]),:]\n",
    "        else:\n",
    "            threshholds = None\n",
    "\n",
    "        # remove source file name\n",
    "        concept = re.sub('\\[.*\\]', '', concept)\n",
    "\n",
    "        # remove whitespace\n",
    "        # concept = concept.replace(' ', '')\n",
    "\n",
    "        # make quantity more legible\n",
    "        quantity = concept.split('_')[-1]\n",
    "        concept = ' '.join(concept.split('_')[:-1]).title()\n",
    "\n",
    "        units = {\n",
    "            'Mean Creatinine' : 'mg/dL',\n",
    "            'Min Creatinine' : 'mg/dL',\n",
    "            'Std Co2' : 'mmol/L',\n",
    "            'Mean Calcium' : 'mg/dL',\n",
    "            'Std Bmi' : 'kg/m$^2$',\n",
    "            'Min Bmi' : 'kg/m$^2$',\n",
    "            'Mean Bmi' : 'kg/m$^2$',\n",
    "            'Latest Pulse' : 'bpm',\n",
    "            'Std Pulse' : 'bpm',\n",
    "            'Mean Pulse' : 'bpm',\n",
    "            'Min Height Cm' : 'cm',\n",
    "            'Mean Temp' : 'F',\n",
    "            'Std Age' : 'years',\n",
    "            'Mean Bp Dia' : 'mmHg',\n",
    "            'Mean Bp Sys' : 'mmHg',\n",
    "            'Latest Bp Sys' : 'mmHg',\n",
    "        }\n",
    "\n",
    "        if quantity == 'level0':\n",
    "            concept = 'No ' + concept\n",
    "        elif phenotype:\n",
    "            if ' ' in concept:\n",
    "                phe_number = concept.split(' ')[0]\n",
    "                phe_type = concept.split(' ')[1]\n",
    "            else:\n",
    "                phe_number = concept\n",
    "                phe_type = 'Dx/Rx'\n",
    "            concept = phe_type + ' phenotype ' + phe_number + ' is ' + quantity\n",
    "        elif quantity == 'level1':\n",
    "            concept = concept\n",
    "        elif quantity == 'low':\n",
    "            t = str(round((threshholds['medium_left'] + threshholds['low'])/2, 3)) + ' ' + units[concept]\n",
    "            quantity = f'low (<{t})'\n",
    "            concept = concept + ' is ' + quantity\n",
    "        elif quantity == 'medium':\n",
    "            t1 = str(round(threshholds['medium_left'], 3))\n",
    "            t2 = str(round(threshholds['medium_right'], 3))\n",
    "            quantity = f'medium ({t1} - {t2} {units[concept]})'\n",
    "            concept = concept + ' is ' + quantity\n",
    "        elif quantity == 'high':\n",
    "            t = str(round((threshholds['medium_right'] + threshholds['high'])/2, 3)) + ' ' + units[concept]\n",
    "            quantity = f'high (>{t})'\n",
    "            concept = concept + ' is ' + quantity\n",
    "        else:\n",
    "            concept = concept + ' is ' + quantity\n",
    "\n",
    "\n",
    "        # specific adjustments\n",
    "        swap = {\n",
    "            'Mmhg' : 'mmHg',\n",
    "            'Cvp' : 'CVP',\n",
    "            'Map' : 'MAP',\n",
    "            'Hr' : 'hr',\n",
    "            'Fluid Based' : '(fluid based)',\n",
    "            'Arterial Line' : '(arterial line)',\n",
    "            'Mpv' : 'MPV',\n",
    "            'Bmi' : 'BMI',\n",
    "            'Std' : 'Std dev',\n",
    "            'Bp Dia' : 'diastolic blood pressure',\n",
    "            'Bp Sys' : 'systolic blood pressure',\n",
    "            'Creatinine' : 'creatinine',\n",
    "            'Calcium' : 'calcium',\n",
    "            'Temp' : 'temperature',\n",
    "            'Height Cm' : 'height',\n",
    "            ' Mg' : 'mg',\n",
    "            'Lv' : 'Lab/vital',\n",
    "            'Dxrx' : 'Dx/Rx',\n",
    "            'Age' : 'age',\n",
    "            'Pulse' : 'pulse',\n",
    "            'Co2' : 'CO2',\n",
    "            'Tablet' : 'tablet',\n",
    "            'History' : 'History of',\n",
    "            ' Rx ' : ' ',\n",
    "            'Family Cardiac Hx' : 'family history of cardiac disease',\n",
    "            'Latest Smoking Status Never' : 'latest smoking status of \\'never\\'',\n",
    "        }\n",
    "\n",
    "        for key in swap.keys():\n",
    "            concept = concept.replace(key, swap[key])\n",
    "        \n",
    "\n",
    "        new_concepts.append(concept)\n",
    "\n",
    "    return new_concepts\n",
    "\n",
    "save = False\n",
    "\n",
    "model = models[2]\n",
    "class Params(object):\n",
    "    binary_pos_only = model.binary_pos_only\n",
    "    n_rules = model.n_rules\n",
    "    n_classes = model.n_classes\n",
    "    category_info = dataset['category_info']\n",
    "    epsilon1 = model.min_epsilon1\n",
    "    epsilon2 = model.min_epsilon2\n",
    "    epsilon3 = model.min_epsilon3\n",
    "    feature_names = dataset.get('feature_names')\n",
    "    \n",
    "params = Params()\n",
    "\n",
    "# Extract rule data from a trained model\n",
    "rules_all, attention_mask, connection_mask, encoding_value_details, relation_mat = extract_rules_from_net(\n",
    "    model.estimator, \n",
    "    params, \n",
    "    model.scaler)\n",
    "\n",
    "# Get thresholds\n",
    "pd.options.display.float_format = '{:.6f}'.format\n",
    "encoding = get_thresholds(rules_all, params, encoding_value_details)\n",
    "if save:\n",
    "    encoding.to_csv(f'cv_results/{dir_name}/{i}_encoding.csv')\n",
    "\n",
    "\n",
    "# get rules\n",
    "rules_all = rules_all.drop(columns=['encoding'])\n",
    "\n",
    "# scale concept importance\n",
    "scaler = MinMaxScaler()\n",
    "rules_all.iloc[:-1,:] = scaler.fit_transform(rules_all.iloc[:-1,:].values.reshape(-1, 1)).reshape(rules_all.iloc[:-1,:].shape)\n",
    "\n",
    "# filter rules and concepts\n",
    "x = rules_all.copy()\n",
    "x = x[x.loc['direction',:].sort_values(ascending=False).index.tolist()] # sort rules by importance\n",
    "x.columns = [f'R{i}' for i in range(x.shape[1])] # rename rules by importance \n",
    "x.columns = (x.loc['direction',:].index + '\\n' + x.loc['direction',:].astype(float).round(3).astype(str)).values.tolist() # add rule importance to column names\n",
    "x = x.loc[:, x.loc['direction',:] >= 0.1] # remove rules with low importance\n",
    "x = x.iloc[:-1,:] # remove class and inference contribution rows\n",
    "x = x.astype(float)\n",
    "x = x.loc[x.max(axis=1) >= 0.1,:] # remove concepts without any importance\n",
    "x.index = pub_fig_adjust_index(x.index.tolist(), encoding) # make index more legibel\n",
    "x = sort_index(x)\n",
    "\n",
    "# plot\n",
    "plt.clf()\n",
    "sns.set(font_scale=1.3)\n",
    "plt.figure(figsize=(17,15))\n",
    "g = sns.heatmap(x, cmap='Reds', linewidths=0.5, linecolor='white', cbar_kws={'shrink': 0.5})\n",
    "g.figure.tight_layout()\n",
    "g.set_xticklabels(g.get_xmajorticklabels(), fontweight = 'bold')\n",
    "# plt.title('Rules')\n",
    "\n",
    "if save:\n",
    "    plt.savefig(f'cv_results/{dir_name}/fold_2_rules.png', dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot membership functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 2\n",
    "model = models[i]\n",
    "save_prefix = f'cv_results/{dir_name}/model_{i}_membership_functions/'\n",
    "os.makedirs(save_prefix, exist_ok=True)\n",
    "\n",
    "# Encoding Visualization\n",
    "rule_var_name = rules_all.index\n",
    "rule_var_name = set([name.split('_low')[0] for name in rule_var_name if 'low' in name])\n",
    "\n",
    "\n",
    "continous_variable_name = [params.feature_names[index] \\\n",
    "                            for index in range(len(params.category_info)) \\\n",
    "                            if params.category_info[index]==0]\n",
    "\n",
    "for index, var_name in enumerate(continous_variable_name):\n",
    "    if var_name in rule_var_name:\n",
    "        var_name = var_name.replace('/','backslash')\n",
    "        draw_membership_function(\n",
    "            encoding.iloc[index, 0], \n",
    "            encoding.iloc[index, 1], \n",
    "            encoding.iloc[index, 2], \n",
    "            encoding.iloc[index, 3], \n",
    "            output_path=save_prefix, \n",
    "            n_points=1000, \n",
    "            epsilon=params.epsilon1, \n",
    "            variable_name=var_name\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
