{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the smallest optimal number of features to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mrmr import mrmr_classif\n",
    "from sklearn.impute import KNNImputer\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CASE = 'CASE'\n",
    "ID = 'ID'\n",
    "data_dir = 'data'\n",
    "subdir = 'experiment'\n",
    "\n",
    "def rm_cols(df):\n",
    "    df = df.drop([c for c in df.columns if 'px_' in c], axis=1)\n",
    "    df = df.drop([c for c in df.columns if 'dx_Z' in c], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prep(X_train, X_test, cohort):\n",
    "    # get target\n",
    "    y_train = pd.merge(X_train, cohort, on=ID)[CASE]\n",
    "    y_test = pd.merge(X_test, cohort, on=ID)[CASE]\n",
    "\n",
    "    # get cols with >60% present\n",
    "    col_freq = (X_train.notna().sum() / X_train.shape[0])\n",
    "    col_freq = col_freq[col_freq > 0.6]\n",
    "    freq_cols = col_freq.index\n",
    "    X_train = X_train[freq_cols]\n",
    "\n",
    "    # drop extra columns\n",
    "    cols_to_drop = set(X_test.columns) - set(X_train.columns)\n",
    "    X_test = X_test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def run_mrmr(X_train, X_test, y_train, k):\n",
    "    mrmr_results = mrmr_classif(X=X_train, y=y_train, K=k, show_progress=False, return_scores=True, relevance='rf')\n",
    "    feature_idx = mrmr_results[0]\n",
    "    feature_relevance = mrmr_results[1]\n",
    "    feature_redundancy = mrmr_results[2]\n",
    "    X_train = X_train.loc[:, feature_idx]\n",
    "    X_test = X_test.loc[:, feature_idx]\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def impute(X_train, X_test, cols, n):\n",
    "\n",
    "    def make_bool(df):\n",
    "        cols = [c for c in df.columns if 'dx_' in c or 'px_' in c or 'rx_' in c]\n",
    "        df[cols] = df[cols].fillna(0)\n",
    "        df[cols] = df[cols].apply(lambda x: x.apply(lambda y: 1 if y > 1 else y))\n",
    "        return df\n",
    "\n",
    "    # impute dx, rx, and px variables with 0, replace values >1 with 1.\n",
    "    X_train = make_bool(X_train)\n",
    "    X_test = make_bool(X_test)\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=n, weights='distance')\n",
    "    X_train[cols] = imputer.fit_transform(X_train)\n",
    "    X_test[cols] = imputer.transform(X_test)\n",
    "\n",
    "    # if boolean variable, round to 0 or 1.\n",
    "    binary_features = set(['SMOKING_STATUS', 'ALCOHOL_USE_STATUS', 'ILLICIT_DRUG_USE', 'CARDIAC_HX'])\n",
    "    binary_cols = []\n",
    "    for b in binary_features:\n",
    "        for c in X_train.columns:\n",
    "            if b in c:\n",
    "                binary_cols.append(c)\n",
    "    binary_col_idx = np.array([i for i, e in enumerate(cols) if e in binary_cols])\n",
    "    if binary_col_idx.shape[0] > 0:\n",
    "        X_train.iloc[:,binary_col_idx] = np.round(X_train.iloc[:,binary_col_idx])\n",
    "        X_test.iloc[:,binary_col_idx] = np.round(X_test.iloc[:,binary_col_idx])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def process(X1_train, X1_test, X2_train, X2_test, cohort, k=20, n=5, suffixes=['','']):\n",
    "    if X2_train is not None:\n",
    "        X_train = pd.merge(X1_train, X2_train, on=ID, how='left', suffixes=suffixes) # left because a couple of the tables have too few or too many patients\n",
    "        X_test = pd.merge(X1_test, X2_test, on=ID, how='left', suffixes=suffixes)\n",
    "    else:\n",
    "        X_train = X1_train\n",
    "        X_test = X1_test\n",
    "    X_train = rm_cols(X_train)\n",
    "    X_test = rm_cols(X_test)\n",
    "    X_train, X_test, y_train, y_test = prep(X_train, X_test, cohort)\n",
    "\n",
    "    X_train_idx = X_train.index\n",
    "    X_test_idx = X_test.index\n",
    "    y_train_idx = y_train.index\n",
    "    y_test_idx = y_test.index\n",
    "\n",
    "    X_train, X_test = run_mrmr(\n",
    "        X_train.reset_index(drop=True), \n",
    "        X_test.reset_index(drop=True), \n",
    "        y_train.reset_index(drop=True), \n",
    "        k\n",
    "    )\n",
    "\n",
    "    X_train, X_test = impute(X_train, X_test, X_train.columns, n)\n",
    "\n",
    "    X_train.index = X_train_idx\n",
    "    X_test.index = X_test_idx\n",
    "    y_train.index = y_train_idx\n",
    "    y_test.index = y_test_idx\n",
    "\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def save(X_train, X_test, y_train, y_test, data_dir, name):\n",
    "    os.makedirs(f'{data_dir}/{name}', exist_ok=True)\n",
    "    X_train.to_csv(f'{data_dir}/{name}/X_train.csv')\n",
    "    X_test.to_csv(f'{data_dir}/{name}/X_test.csv')\n",
    "    y_train.to_csv(f'{data_dir}/{name}/y_train.csv')\n",
    "    y_test.to_csv(f'{data_dir}/{name}/y_test.csv')\n",
    "\n",
    "def combine(X1, X2, y):\n",
    "    X = pd.merge(X1, X2, left_index=True, right_index=True)\n",
    "    y = pd.merge(X, y, left_index=True, right_index=True)['CASE']\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def binary_eval(model, x_train, y_train, x_test, y_test):\n",
    "    # evaluate binary\n",
    "    def score(y_pred, y_pred_proba, y_label):\n",
    "        auc = roc_auc_score(y_label, y_pred_proba)\n",
    "        auprc = average_precision_score(y_label, y_pred_proba)\n",
    "        f1 = f1_score(y_label, y_pred)\n",
    "        acc = accuracy_score(y_label, y_pred)\n",
    "        precision = precision_score(y_label, y_pred)\n",
    "        recall = recall_score(y_label, y_pred)\n",
    "        results = [auc, auprc, f1, acc, precision, recall]\n",
    "        conf_mat = confusion_matrix(y_label, y_pred)\n",
    "\n",
    "        return results, conf_mat\n",
    "\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_proba_train = model.predict_proba(x_train)[:,1]\n",
    "\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_proba_test = model.predict_proba(x_test)[:,1]\n",
    "\n",
    "    results_train, conf_mat_train = score(y_pred_train, y_pred_proba_train, y_train)\n",
    "    results_test, conf_mat_test = score(y_pred_test, y_pred_proba_test, y_test)\n",
    "\n",
    "    eval_results = pd.DataFrame(\n",
    "            [results_train, results_test], \n",
    "            columns=['auc', 'auprc', 'f1', 'acc', 'precision', 'recall'],\n",
    "            index=['train', 'test'])\n",
    "\n",
    "    # feature importance\n",
    "    feature_scores = pd.DataFrame(model.feature_importances_, index=x_train.columns, columns=['importance']).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    return eval_results, [conf_mat_train,conf_mat_test], feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "cohort = pd.read_csv(f'{data_dir}/cohort.csv', index_col=0)\n",
    "\n",
    "# latest\n",
    "X_latest_train = pd.read_csv(f'{data_dir}/X_latest_train.csv', index_col=0)\n",
    "X_latest_test = pd.read_csv(f'{data_dir}/X_latest_test.csv', index_col=0)\n",
    "\n",
    "# demographics\n",
    "S_train = pd.read_csv(f'{data_dir}/S_train.csv', index_col=0)\n",
    "S_test = pd.read_csv(f'{data_dir}/S_test.csv', index_col=0)\n",
    "\n",
    "# labs vitals stats\n",
    "X_cont_stats_train = pd.read_csv(f'{data_dir}/X_cont_stats_train.csv', index_col=0)\n",
    "X_cont_stats_test = pd.read_csv(f'{data_dir}/X_cont_stats_test.csv', index_col=0)\n",
    "\n",
    "# Dx Rx one hot encoded\n",
    "X_dx_rx_px_history_train = pd.read_csv(f'{data_dir}/X_dx_rx_px_history_train.csv', index_col=0)\n",
    "X_dx_rx_px_history_test = pd.read_csv(f'{data_dir}/X_dx_rx_px_history_test.csv', index_col=0)\n",
    "\n",
    "# Dx Rx phenotypes\n",
    "pheno_dir = f'{data_dir}/phenotypes_50_dxrx_HALS-exact'\n",
    "X_train_dxrx = pd.read_csv(f'{pheno_dir}/pheno_patient_membership_train.csv', index_col=0)\n",
    "X_test_dxrx = pd.read_csv(f'{pheno_dir}/pheno_patient_membership_test.csv', index_col=0)\n",
    "\n",
    "# labs vitals phenotypes\n",
    "pheno_dir = f'{data_dir}/phenotypes_30_lv_HALS-exact'\n",
    "X_train_lv = pd.read_csv(f'{pheno_dir}/pheno_patient_membership_train.csv', index_col=0)\n",
    "X_test_lv = pd.read_csv(f'{pheno_dir}/pheno_patient_membership_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dxrx.columns = [f'{c}_dxrx' for c in X_train_dxrx.columns]\n",
    "X_test_dxrx.columns = [f'{c}_dxrx' for c in X_test_dxrx.columns]\n",
    "X_train_lv.columns = [f'{c}_lv' for c in X_train_lv.columns]\n",
    "X_test_lv.columns = [f'{c}_lv' for c in X_test_lv.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute feature importance/relevance score in mRMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = None\n",
    "X1_train = X_train_dxrx\n",
    "X1_test = X_test_dxrx\n",
    "X2_train = X_train_lv\n",
    "X2_test = X_test_lv\n",
    "\n",
    "\n",
    "X_train = pd.merge(X1_train, X2_train, on=ID, how='left', suffixes=suffixes) # left because a couple of the tables have too few or too many patients\n",
    "X_test = pd.merge(X1_test, X2_test, on=ID, how='left', suffixes=suffixes)\n",
    "X_train = rm_cols(X_train)\n",
    "X_test = rm_cols(X_test)\n",
    "X_train, X_test, y_train, y_test = prep(X_train, X_test, cohort)\n",
    "\n",
    "mrmr_results = mrmr_classif(X=X_train, y=y_train, K=10, show_progress=False, return_scores=True, relevance='rf')\n",
    "feature_idx = mrmr_results[0]\n",
    "feature_relevance = mrmr_results[1]\n",
    "feature_redundancy = mrmr_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train[feature_idx], y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "results = []\n",
    "for i in range(3):\n",
    "    model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "    model.fit(X_train, y_train)\n",
    "    eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "    eval_results['i'] = i\n",
    "    results.append(eval_results)\n",
    "\n",
    "results = pd.concat(results)\n",
    "results = results.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latest + demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, kolmogorov-smirnov)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, f-statistic)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, RF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, kolmogorov-smirnov)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, f-statistic)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, RF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, kolmogorov-smirnov)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, F-statistic)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(feature_relevance.values.shape[0]), y=feature_relevance.sort_values(ascending=False).values)\n",
    "plt.ylabel('Feature Relevance')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.title('Feature Relevance (mRMR, RF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance of different number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = []\n",
    "\n",
    "for k in range(10,81,10):\n",
    "    X_train, _, y_train, _ = process(X_train_dxrx, X_test_dxrx, X_train_lv, X_test_lv, cohort, suffixes=['_dxrx', '_lv'], k=k) # process data, mRMR, impute\n",
    "    # X_train, _, y_train, _ = process(X_latest_train, X_latest_test, S_train, S_test, cohort, k=k)\n",
    "    # X_train, _, y_train, _ = process(X_dx_rx_px_history_train, X_dx_rx_px_history_test, X_cont_stats_train, X_cont_stats_test, cohort, k=k)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "    # compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "    class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "    for i in range(3):\n",
    "        model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "        model.fit(X_train, y_train)\n",
    "        eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "        eval_results['k'] = k\n",
    "        eval_results['i'] = i\n",
    "        results_1.append(eval_results)\n",
    "\n",
    "results_1 = pd.concat(results_1)\n",
    "results_1 = results_1.reset_index()\n",
    "results_1['index'] = results_1['index'].str.replace('test','valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = results_1.rename(columns={\n",
    "    'f1' : 'F1',\n",
    "    'auc' : 'AUROC',\n",
    "    'auprc' : 'AUPRC',\n",
    "    'k' : 'Number of Features'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.lineplot(data=results_1, x='Number of Features', y='AUROC', hue='index')\n",
    "g.figure.tight_layout()\n",
    "g.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['auc', 'auprc', 'f1', 'acc', 'precision', 'recall']\n",
    "\n",
    "# Create a figure and 6 subplots, arranged in a 2x3 grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the 2D array to 1D for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.lineplot(data=results_1, x='k', y=metric, hue='index', ax=ax)\n",
    "    ax.legend(title='Type')\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('RF performance with different number of phenotype features (k) selected by mRMR (mean and 95% conf int of 3 replicates)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = []\n",
    "\n",
    "for k in range(10,81,10):\n",
    "    # X_train, _, y_train, _ = process(X_train_dxrx, X_test_dxrx, X_train_lv, X_test_lv, cohort, suffixes=['_dxrx', '_lv'], k=k) # process data, mRMR, impute\n",
    "    X_train, _, y_train, _ = process(X_latest_train, X_latest_test, S_train, S_test, cohort, k=k)\n",
    "    # X_train, _, y_train, _ = process(X_dx_rx_px_history_train, X_dx_rx_px_history_test, X_cont_stats_train, X_cont_stats_test, cohort, k=k)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "    # compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "    class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "    for i in range(3):\n",
    "        model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "        model.fit(X_train, y_train)\n",
    "        eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "        eval_results['k'] = k\n",
    "        eval_results['i'] = i\n",
    "        results_2.append(eval_results)\n",
    "\n",
    "results_2 = pd.concat(results_2)\n",
    "results_2 = results_2.reset_index()\n",
    "results_2['index'] = results_2['index'].str.replace('test','valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = results_2.rename(columns={\n",
    "    'f1' : 'F1',\n",
    "    'auc' : 'AUROC',\n",
    "    'auprc' : 'AUPRC',\n",
    "    'k' : 'Number of Features'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.lineplot(data=results_2, x='Number of Features', y='F1', hue='index')\n",
    "g.figure.tight_layout()\n",
    "g.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['auc', 'auprc', 'f1', 'acc', 'precision', 'recall']\n",
    "\n",
    "# Create a figure and 6 subplots, arranged in a 2x3 grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the 2D array to 1D for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.lineplot(data=results_2, x='k', y=metric, hue='index', ax=ax)\n",
    "    ax.legend(title='Type')\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('RF performance with different number of latest+demo features (k) selected by mRMR (mean and 95% conf int of 3 replicates)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3 = []\n",
    "\n",
    "for k in range(10,81,10):\n",
    "    # X_train, _, y_train, _ = process(X_train_dxrx, X_test_dxrx, X_train_lv, X_test_lv, cohort, suffixes=['_dxrx', '_lv'], k=k) # process data, mRMR, impute\n",
    "    # X_train, _, y_train, _ = process(X_latest_train, X_latest_test, S_train, S_test, cohort, k=k)\n",
    "    X_train, _, y_train, _ = process(X_dx_rx_px_history_train, X_dx_rx_px_history_test, X_cont_stats_train, X_cont_stats_test, cohort, k=k)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "    # compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "    class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "    for i in range(3):\n",
    "        model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "        model.fit(X_train, y_train)\n",
    "        eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "        eval_results['k'] = k\n",
    "        eval_results['i'] = i\n",
    "        results_3.append(eval_results)\n",
    "\n",
    "results_3 = pd.concat(results_3)\n",
    "results_3 = results_3.reset_index()\n",
    "results_3['index'] = results_3['index'].str.replace('test','valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3 = results_3.rename(columns={\n",
    "    'f1' : 'F1',\n",
    "    'auc' : 'AUROC',\n",
    "    'auprc' : 'AUPRC',\n",
    "    'k' : 'Number of Features'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.lineplot(data=results_3, x='Number of Features', y='AUROC', hue='index')\n",
    "g.figure.tight_layout()\n",
    "g.legend_.remove()\n",
    "plt.savefig('ami/ami_180d_pxFalse_stringent-matchTrue_cntrl2_v2/sum-stats_n-features_auroc.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['auc', 'auprc', 'f1', 'acc', 'precision', 'recall']\n",
    "\n",
    "# Create a figure and 6 subplots, arranged in a 2x3 grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the 2D array to 1D for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.lineplot(data=results_3, x='k', y=metric, hue='index', ax=ax)\n",
    "    ax.legend(title='Type')\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('RF performance with different number of summary stat features (k) selected by mRMR (mean and 95% conf int of 3 replicates)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_csv('data/cohort.csv', index_col=0)\n",
    "\n",
    "data_path = 'data/experiment/'\n",
    "\n",
    "X_ld_train = pd.read_csv(f'{data_path}/20_latest+demo/X_train.csv', index_col=0)\n",
    "X_ld_test = pd.read_csv(f'{data_path}/20_latest+demo/X_test.csv', index_col=0)\n",
    "y_ld_train = pd.read_csv(f'{data_path}/20_latest+demo/y_train.csv', index_col=0)\n",
    "y_ld_test = pd.read_csv(f'{data_path}/20_latest+demo/y_test.csv', index_col=0)\n",
    "\n",
    "X_agg_train = pd.read_csv(f'{data_path}/30_aggregate/X_train.csv', index_col=0)\n",
    "X_agg_test = pd.read_csv(f'{data_path}/30_aggregate/X_test.csv', index_col=0)\n",
    "\n",
    "X_phe_train = pd.read_csv(f'{data_path}/30_phenotypes/X_train.csv', index_col=0)\n",
    "X_phe_test = pd.read_csv(f'{data_path}/30_phenotypes/X_test.csv', index_col=0)\n",
    "\n",
    "\n",
    "X_latest_demo_agg_train, y_latest_demo_agg_train = combine(X_ld_train, X_agg_train, y_ld_train)\n",
    "X_latest_demo_agg_test, y_latest_demo_agg_test = combine(X_ld_test, X_agg_train, y_ld_test)\n",
    "# X_latest_demo_agg_train, X_latest_demo_agg_test = run_mrmr(\n",
    "#     X_latest_demo_agg_train.reset_index(drop=True), \n",
    "#     X_latest_demo_agg_test.reset_index(drop=True), \n",
    "#     y_latest_demo_agg_train.reset_index(drop=True), \n",
    "#     k)\n",
    "\n",
    "X_latest_demo_phe_train, y_latest_demo_phe_train = combine(X_ld_train, X_phe_train, y_ld_train)\n",
    "X_latest_demo_phe_test, y_latest_demo_phe_test = combine(X_ld_test, X_phe_test, y_ld_test)\n",
    "# X_latest_demo_phe_train, X_latest_demo_phe_test = run_mrmr(\n",
    "#     X_latest_demo_phe_train.reset_index(drop=True), \n",
    "#     X_latest_demo_phe_test.reset_index(drop=True), \n",
    "#     y_latest_demo_phe_train.reset_index(drop=True), \n",
    "#     k)\n",
    "\n",
    "\n",
    "# combine all\n",
    "X_all_train = pd.merge(X_ld_train, X_agg_train, left_index=True, right_index=True)\n",
    "X_all_train = pd.merge(X_all_train, X_phe_train, left_index=True, right_index=True)\n",
    "y_all_train = pd.merge(X_all_train, y_ld_train, left_index=True, right_index=True)['CASE']\n",
    "X_all_test = pd.merge(X_ld_test, X_agg_test, left_index=True, right_index=True)\n",
    "X_all_test = pd.merge(X_all_test, X_phe_test, left_index=True, right_index=True)\n",
    "y_all_test = pd.merge(X_all_test, y_ld_test, left_index=True, right_index=True)['CASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4 = []\n",
    "\n",
    "for k in range(10,51,10):\n",
    "    X_train, _, y_train, _ = process(X_ld_train, X_ld_test, X_agg_train, X_agg_test, cohort, k=k)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "    # compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "    class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "    for i in range(3):\n",
    "        model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "        model.fit(X_train, y_train)\n",
    "        eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "        eval_results['k'] = k\n",
    "        eval_results['i'] = i\n",
    "        results_4.append(eval_results)\n",
    "\n",
    "results_4 = pd.concat(results_4)\n",
    "results_4 = results_4.reset_index()\n",
    "results_4['index'] = results_4['index'].str.replace('test','valid')\n",
    "\n",
    "results_4 = results_4.rename(columns={\n",
    "    'f1' : 'F1',\n",
    "    'auc' : 'AUROC',\n",
    "    'auprc' : 'AUPRC',\n",
    "    'k' : 'Number of Features'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.lineplot(data=results_4, x='Number of Features', y='F1', hue='index')\n",
    "g.figure.tight_layout()\n",
    "g.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUROC', 'AUPRC', 'F1', 'acc', 'precision', 'recall']\n",
    "\n",
    "# Create a figure and 6 subplots, arranged in a 2x3 grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the 2D array to 1D for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.lineplot(data=results_4, x='Number of Features', y=metric, hue='index', ax=ax)\n",
    "    ax.legend(title='Type')\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('RF performance with different number of features (k) selected by mRMR (mean and 95% conf int of 3 replicates)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_5 = []\n",
    "\n",
    "for k in range(10,51,10):\n",
    "    X_train, _, y_train, _ = process(X_ld_train, X_ld_test, X_phe_train, X_phe_test, cohort, k=k)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "    # compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "    class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "    for i in range(3):\n",
    "        model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "        model.fit(X_train, y_train)\n",
    "        eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "        eval_results['k'] = k\n",
    "        eval_results['i'] = i\n",
    "        results_5.append(eval_results)\n",
    "\n",
    "results_5 = pd.concat(results_5)\n",
    "results_5 = results_5.reset_index()\n",
    "results_5['index'] = results_5['index'].str.replace('test','valid')\n",
    "\n",
    "results_5 = results_5.rename(columns={\n",
    "    'f1' : 'F1',\n",
    "    'auc' : 'AUROC',\n",
    "    'auprc' : 'AUPRC',\n",
    "    'k' : 'Number of Features'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.lineplot(data=results_5, x='Number of Features', y='AUROC', hue='index')\n",
    "g.figure.tight_layout()\n",
    "g.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUROC', 'AUPRC', 'F1', 'acc', 'precision', 'recall']\n",
    "\n",
    "# Create a figure and 6 subplots, arranged in a 2x3 grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the 2D array to 1D for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.lineplot(data=results_5, x='Number of Features', y=metric, hue='index', ax=ax)\n",
    "    ax.legend(title='Type')\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('RF performance with different number of features (k) selected by mRMR (mean and 95% conf int of 3 replicates)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_6 = []\n",
    "\n",
    "for k in range(10,81,10):\n",
    "    X_train, _, y_train, _ = process(X_all_train, X_all_test, None, None, cohort, k=k)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) # split training set into train and valid\n",
    "\n",
    "    # compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train.values.reshape(-1))\n",
    "    class_weights = dict(zip(np.unique(np.unique(y_train)), class_weights))\n",
    "\n",
    "    for i in range(3):\n",
    "        model = RandomForestClassifier(n_estimators=500, class_weight=class_weights, max_depth=5) # train model\n",
    "        model.fit(X_train, y_train)\n",
    "        eval_results, conf_mat, feature_scores = binary_eval(model, X_train, y_train, X_valid, y_valid) # evaluate model\n",
    "        eval_results['k'] = k\n",
    "        eval_results['i'] = i\n",
    "        results_6.append(eval_results)\n",
    "\n",
    "results_6 = pd.concat(results_6)\n",
    "results_6 = results_6.reset_index()\n",
    "results_6['index'] = results_6['index'].str.replace('test','valid')\n",
    "\n",
    "results_6 = results_6.rename(columns={\n",
    "    'f1' : 'F1',\n",
    "    'auc' : 'AUROC',\n",
    "    'auprc' : 'AUPRC',\n",
    "    'k' : 'Number of Features'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "g = sns.lineplot(data=results_6, x='Number of Features', y='F1', hue='index')\n",
    "g.figure.tight_layout()\n",
    "g.legend_.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['AUROC', 'AUPRC', 'F1', 'acc', 'precision', 'recall']\n",
    "\n",
    "# Create a figure and 6 subplots, arranged in a 2x3 grid\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))  # Adjust the size as needed\n",
    "\n",
    "# Flatten the 2D array to 1D for iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    sns.lineplot(data=results_6, x='Number of Features', y=metric, hue='index', ax=ax)\n",
    "    ax.legend(title='Type')\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.suptitle('RF performance with different number of features (k) selected by mRMR (mean and 95% conf int of 3 replicates)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
